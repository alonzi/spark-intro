{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up your spark environment\n",
    "We are going to use the [pyspark api](https://spark.apache.org/docs/2.3.1/quick-start.html), you can also use Scala or R or something else.\n",
    "\n",
    "## In this Noteboook\n",
    "1. create a context\n",
    "2. learn dataframe fundamentals\n",
    "3. read data into dataframe from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT RUN\n",
    "### DO NOT RUN\n",
    "### DO NOT RUN\n",
    "\n",
    "# Initialize the spark environment (takes ~ 1min)\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf().setAppName('odl').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlc = pyspark.sql.SQLContext(sc)\n",
    "\n",
    "### DO NOT RUN\n",
    "### DO NOT RUN\n",
    "### DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.103.75:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>odl</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=odl>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f6b9c127048>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "Dataframes are a set of instructions to create a dataset\n",
    "\n",
    "* NB: In R and python dataframes actually contain the data and take up lots of space in memory\n",
    "* NB: In spark dataframes are a set of instructions and take up no space in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "firstDataFrame = sc.range(1000000)\n",
    "print(firstDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(firstDataFrame.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see, we had to use the \"take\" method to actually run the instructions and get data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"data.csv\"\n",
    "df = sqlc.read.format(\"csv\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[alpha: int, beta: int, gamma: int]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(alpha=1, beta=2, gamma=3), Row(alpha=1, beta=4, gamma=9)]\n"
     ]
    }
   ],
   "source": [
    "print(df.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- alpha: integer (nullable = true)\n",
      " |-- beta: integer (nullable = true)\n",
      " |-- gamma: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(df.take(5)[0][0])\n",
    "print(df.take(5)[1][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
