{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up your spark environment\n",
    "We are going to use the [pyspark api](https://spark.apache.org/docs/2.3.1/quick-start.html), you can also use Scala or R or something else.\n",
    "\n",
    "## In this Noteboook\n",
    "1. create a context\n",
    "2. learn dataframe fundamentals\n",
    "3. read data into dataframe from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT RUN\n",
    "### DO NOT RUN\n",
    "### DO NOT RUN\n",
    "\n",
    "# Initialize the spark environment (takes ~ 1min)\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf().setAppName('odl').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlc = pyspark.sql.SQLContext(sc)\n",
    "\n",
    "### DO NOT RUN\n",
    "### DO NOT RUN\n",
    "### DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "Dataframes are a set of instructions to create a dataset\n",
    "\n",
    "* NB: In R and python dataframes actually contain the data and take up lots of space in memory\n",
    "* NB: In spark dataframes are a set of instructions and take up no space in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstDataFrame = sc.range(1000000)\n",
    "print(firstDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(firstDataFrame.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see, we had to use the \"take\" method to actually run the instructions and get data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"data.csv\"\n",
    "df = sqlc.read.format(\"csv\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.take(5)[0][0])\n",
    "print(df.take(5)[1][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
